# -*- coding: utf-8 -*-
"""[HW01] AutoEncoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DfKWvgbYOI2C_-Kv8YhfKAML-idSkVaP

# 📝 과제 제출 정보  

| 📌 항목  | ✏️ 입력 |
|---------|-------|
| **학과** | `소프트웨어학부` |
| **학번** | `20203084` |
| **이름** | `송나단` |

🚨 위 과제 제출 정보를 정확히 기입하지 않으면 감점됩니다.
"""

from google.colab import drive
drive.mount('/content/drive')

"""---

## [HW01] AutoEncoder

다음을 확인하는 리포트를 Colab 노트북을 이용해 작성한다.

1. 정규분포를 통한 파라미터 초기화를 진행한 `Linear` layer를 Xavier 초기화, He 초기화 기법을 통해 초기화를 진행한 뒤, 결과에 대해 분석한다.

2. Autoencoder의 encoder를 거쳐 나온 representation $z$를 groud truth(GT) label별로 다른색을 주어 $z$의 분포를 가시화한다.

3. $z$의 분포를 보고 encoder가 label별로 discriminative한 representation을 만들어내는지 확인한다.

4. 만일 discriminative하지 않다면, autoencoder가 discriminative한 representation $z$를 학습하도록 $z$의 차원을 바꿔가면서 실험해본다.

5. $z$ 공간의 임의의 위치를 sampling한 후, 학습된 decoder의 입력으로 주는 방식으로 generative model을 구축한다. 의미있게 만들어진 이미지는 어떤 특징을 갖고 있는지 논의한다.

---

# 오토인코더 (Pytorch)
Pytorch에서 제공하는 고수준 API를 이용해, 오토인코더(autoencoder)를 구현한다.

참고문헌
* [Pytorch tutorial](https://tutorials.pytorch.kr/)
* [Dive into Deep learning](https://d2l.ai/)


주의사항
* Colab에서 코드에 이상이 없음에도 불구하고 결과가 제대로 나오지 않을 경우, '런타임 다시 시작...'을 해보도록 한다.'


## Deep Neural Network 기초
다음 비디오를 보고 심층신경망(deep neural network) 기반 딥러닝 기법은 이해하도록 한다.
* [신경망이란 무엇인가? | 1장.딥러닝에 관하여 (3Blue1Brown)](https://youtu.be/aircAruvnKk)
* [경사 하강, 신경 네트워크가 학습하는 방법 | 심층 학습, 2장 (3Blue1Brown)](https://youtu.be/IHZwWFHWa-w)
* [What is backpropagation really doing? | Deep learning, chapter 3 (3Blue1Brown)](https://youtu.be/Ilg3gGewQ5U)
* [Backpropagation calculus | Deep learning, chapter 4 (3Blue1Brown)](https://youtu.be/tIeHLnjs5U8)

## 필요한 모듈 임포트
"""

import sys
import time
import numpy as np
import matplotlib.pyplot as plt

import torch                                      # 파이토치 임포트
import torch.nn as nn                             # nn 모듈 임포트
import torch.nn.functional as F

import torchvision                                # torchvision 임포트
import torchvision.utils as utils
import torchvision.transforms as transforms       # numpy 이미지에서 tensor 이미지로 변경하기 위한 모듈
import torchvision.datasets as datasets           # pytorch에 내장된 dataset을 불러오기 위한 모듈

from tqdm import tqdm

print("Python:", sys.version)
print("Numpy:", np.__version__)

print("Torch: ", torch.__version__)               # 파이토치 버전을 확인하도록 한다.
print("Torchvision: ", torchvision.__version__)   # 토치비전 버전을 확인하도록 한다.

"""## GPU 디바이스 선택 및 확인"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
use_cuda = torch.cuda.is_available()
print(use_cuda)
if use_cuda:
  print(torch.cuda.get_device_name(0))

"""## Hyperparameter 설정
* batch size, learning rate, epoch을 설정
"""

batch_size    = 100
learning_rate = 0.001
num_epochs    = 10

"""## MNIST 데이터셋 띄우기
![MnistExamples.png](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)
* mnist 데이터셋은 숫자(digit) 손글씨 데이터셋이다.
* 60,000개의 트레이닝 데이터와 10,000개의 테스트 데이터로 이루어져 있다.
* LeCun이 자신의 연구에서 활용하고 배포한 데이터셋이 있으나 최근 보편적으로 많이 활용하기 때문에 텐서플로우나 파이토치 등에서 built-in 데이터셋의 형태로 제공해 주기도 한다.  

### MNIST 이미지 데이터
* MNIST 데이터셋은 여러 버젼이 있으나, 여기서는 이를 파이토치에 맞게 정리하여 torchvision.datasets에서 built-in으로 제공하는 데이터셋을 쓰도록 하겠다. ([link](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html))
  + (PIL image, integer label)로 구성된 데이터를 제공.
  + PIL image는 각 픽셀이 [0, 255] 사이값으로 구성된 H x W x C 크기의 이미지이다.
* 뉴럴넷의 입력으로 넣기 위해 픽셀값이 0 근처에 있는 작은 실수값을 가지도록 다음과 같이 변환한다.
  + ToTensor(): [0, 255] 사이 값을 [0.0, 1.0] 사이값으로 변화시킴과 동시에 C x H x W 형태로 변환시킨다.
"""

import torchvision.transforms as transforms
from torchvision.datasets import MNIST

# download path 정의
download_root = './MNIST_DATASET'


# PIL image (H x W x C) -> torch tensor (C x H x W) and
# [0, 255] -> [0.0, 1.0]
mnist_transform = transforms.Compose([
    transforms.ToTensor(),
])

train_dataset = MNIST(download_root,
                      transform=mnist_transform,
                      train=True,
                      download=True)

test_dataset = MNIST(download_root,
                     transform=mnist_transform,
                     train=False,
                     download=False)

"""## 데이터셋 확인
데이터셋을 로딩한 후 **데이터셋 구성**이 어떻게 되어 있는지 **반드시 확인**하는 습관을 가지도록 한다.

torchvision.datasets.MNIST 데이터셋의 각 데이터는 (이미지, 레이블)로 구성된 튜플이다.

다음은 MNIST 데이터셋에 있는 이미지와 레이블에 대한 각종 정보를 확인하는 코드이다.

해당 코드를 통해 다음 사항을 확인할 수 있다.
* train_dataset의 길이는 60,000이다.
* test_dataset의 길이는 10,000이다.

### MNIST 이미지 데이터
* 각 이미지는 C X H x W 순서로 구성된 [1, 28, 28] 모양의 텐서로 로딩되었다.
* 각 이미지 픽셀값은 ToTensor() 변환을 통해 [0.0, 1.0] 사이의 값으로 변환되었다.
* 각 이미지 픽셀값은 `torch.float32` 타입이다.
* 각 레이블 값의 타입은 파이썬 `int` 형이다.

### MNIST 라벨 데이터
* 각 레이블 값은 [0, 9] 사이의 파이썬 `int`형이다.
"""

def print_MNIST_dataset_info(dataset):
  print(">>> dataset length: ", len(dataset))
  print(">>> type of each data: ", type(dataset[0]))
  first_img, first_label = dataset[0]
  print(">>> image shape: ", first_img.shape)
  print(">>> image dtype: ", first_img.dtype)
  print(">>> image pixel min-value: ", first_img.min())
  print(">>> image pixel max-value: ", first_img.max())
  print(">>> label data type: ", type(first_label))

print("train dataset")
print_MNIST_dataset_info(train_dataset)

print("test dataset")
print_MNIST_dataset_info(test_dataset)

"""## MNIST DataLoader 정의

[파이토치 데이터로더](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)는 데이터셋을 배치단위로 묶어 순회(iteration)할 수 있도록 한다.

아래 코드는 데이터셋에 대한 데이터로더를 만들고, 각 배치가 어떤 텐서 형태로 구성되어 있는지 확인하는 코드이다.
* 데이터로더에서는 이미지들이 배치단위로 묶였기 때문에, B x C x H x W 모양(shape)의 텐서이며, 데이터타입(dtype)은 `torch.float32`가 되었음을 확인하자.
* 데이터로더에서는 레이블들이 배치단위로 묶였기 때문에, B 모양(shape)의 텐서이며, 데이터타입(dtype)은 `torch.int64`가 되었음을 확인하자.

train_dataset과 test_dataset은 동일 형태이므로, train_dataset의 데이터로더에 대해서만 확인하였다.

"""

from torch.utils.data import DataLoader

# dataloader 정의
train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=True)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=batch_size,
                         shuffle=False)

def print_MNIST_data_loader_info(data_loader):
  print(">>> dataset length: ", len(data_loader))
  print(">>> batch_size: ", data_loader.batch_size)
  batch_images, batch_labels = next(iter(train_loader))
  # batch size로 묶은 data 이미지의 형태 확인
  print(">>> batch_images type", type(batch_images))
  print(">>> batch_images shape", batch_images.shape)
  print(">>> batch_images dtype", batch_images.dtype)

print("train dataset")
print_MNIST_data_loader_info(train_loader)

print("test dataset")
print_MNIST_data_loader_info(test_loader)

"""### 첫번째 배치의 첫번째 이미지와 레이블 확인
첫 이미지와 해당 레이블을 찍어서 확인해 보자.

C x H x W 이미지 정보가 보관된 파이토치 텐서를 H x W x C 형태의 PIL 이미지로 변환시키는 편리한 방법은 `to_pil_images` 함수를 사용하는 것이다. 이 예제에서는 픽셀값이 [0.0, 1.0] 값으로 normalize되어 있으므로, `to_pil_images` 함수를 호출하면 [0, 255] 값으로 변환된다.
"""

from torchvision.transforms.functional import to_pil_image
import matplotlib.pyplot as plt

# 첫번째 이미지 한장에 대한 확인
batch_images, batch_labels = next(iter(train_loader))

img = batch_images[0]

plt.figure()
plt.imshow(to_pil_image(img), cmap='gray')

"""## 네트워크 모델 설계
* 인코더 모델: `torch.nn.Sequential` 모듈로 모델로 설계
  + InputLayer로 (1,28,28) 영상을 받고, Linear 레이어를 여럿 통과시킨 후, 출력으로 n_dim차원 벡터가 나오도록 함.
* 디코더 모델: `torch.nn.Sequential` 모듈로 모델로 설계
  + InputLayer에서 n_dim차원 벡터를 받고, Linear 레이어를 여럿 통과시킨 후, 출력으로 (1,28,28) 영상이 나오도록 함.
* 오토인코더 모델: 인코더, 디코더를 결합하여 설계

잠재 벡터(latent vector) $z$의 크기 설정
여기서는 n_dim을 우선 2로 설정한다.
* 즉, n_dim=2
"""

n_dim = 2

"""인코더 모델 정의
* (1, 28, 28) 영상을 입력으로 받도록 입력 레이어 정의
* Flatten으로 입력 텐서를 784-vector로 벡터라이즈
* Fully connected layer로 784 > 256 > 128 > 32 > n_dim 로 차원 축소
"""

enc = nn.Sequential(
  nn.Flatten(),    # 784 = 1 x 28 x 28
  nn.Linear(784, 256),
  nn.ReLU(),
  nn.Linear(256, 128),
  nn.ReLU(),
  nn.Linear(128, 32),
  nn.ReLU(),
  nn.Linear(32, n_dim)
)

enc = enc.to(device)

print(enc)

"""디코더 모델 정의
* Fully connected layey로 n_dim > 32 > 128 > 256 > 784로 차원 확대
* 784-vector를 Reshape을 통해 (28, 28)의 텐서로 변환
"""

dec = nn.Sequential(
    nn.Linear(n_dim, 32),
    nn.ReLU(),
    nn.Linear(32, 256),
    nn.ReLU(),
    nn.Linear(256, 784),
    nn.Unflatten(1, torch.Size([1, 28, 28])),
    nn.Sigmoid(),
)

dec = dec.to(device)

print(dec)

"""AutoEncoder 모델 정의
* (1, 28, 28) 영상을 입력으로 받도록 입력 레이어 정의
* 입력 레이어 > 인코더 > 디코더로 구성
* (1, 28, 28) 영상이 출력되로록 아웃풋 레이어 정의
"""

ae = nn.Sequential(
    enc,
    dec,
)

ae = ae.to(device)

print(ae)

"""## 파라미터 초기화

## TODO

1. encoder, decoder 부분을 수정하여 `Xavier` 또는 `He Initialization`을 적용하여 파라미터를 초기화한 후 학습을 진행해본다. [Xavier, He initialization 설명](https://reniew.github.io/13/)
  * 힌트: `nn.init()`모듈을 살펴본다.

2. 결과에 대한 분석을 작성한다.
"""

import torch.nn as nn

n_dim = 2
class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.enc = nn.Sequential(
            nn.Flatten(),               # 784 = 1 x 28 x 28
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 32),
            nn.ReLU(),
            nn.Linear(32, n_dim)
        )

    def forward(self, x):
        return self.enc(x)

class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
          nn.Linear(n_dim, 32),
          nn.ReLU(),
          nn.Linear(32, 256),
          nn.ReLU(),
          nn.Linear(256, 784),
          nn.Unflatten(1, torch.Size([1, 28, 28])),
          nn.Sigmoid(),
        )

    def forward(self, z):
        return self.layers(z)


class AutoEncoder(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, x):
        z = self.encoder(x)
        out = self.decoder(z)
        return out

## Xavier initialization 으로 파라미터 초기화
def initialize_weights_xavier(m):
  if isinstance(m, nn.Linear):
      nn.init.xavier_uniform_(m.weight)
      nn.init.constant_(m.bias, 0)

def initialize_weights_xavier_n(m):
  if isinstance(m, nn.Linear):
      nn.init.xavier_normal_(m.weight)
      nn.init.constant_(m.bias, 0)

# weight 수집
def get_weight_list(model):
  weights_list = []
  for layer in model.modules():
    if isinstance(layer, nn.Linear):
      weights = layer.weight.data.cpu().flatten().numpy()
      weights_list.append(weights)
  return weights_list

# weight 시각화 함수
def plot_weight_list(weights_list, title="Xavier Weights"):
    plt.figure(figsize=(len(weights_list) * 3, 4))
    for i, weights in enumerate(weights_list):
        plt.subplot(1, len(weights_list), i+1)
        plt.hist(weights, bins=50, density=True)
        plt.title(f"Layer {i+1}")
        plt.xticks([])
        plt.yticks([])
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# 통계 출력 함수
def print_weight_stats(weights_list):
    for i, w in enumerate(weights_list):
        print(f"[Layer {i+1}] mean: {np.mean(w):.4f}, std: {np.std(w):.4f}, min: {np.min(w):.4f}, max: {np.max(w):.4f}")

# Xavier Uniform 초기화
encoder_xavier_u = Encoder()
encoder_xavier_u.apply(initialize_weights_xavier)

weights_xavier_u = get_weight_list(encoder_xavier_u)
plot_weight_list(weights_xavier_u, "Xavier Initialization - Uniform")
print_weight_stats(weights_xavier_u)

# Xavier Normal 초기화
encoder_xavier_n = Encoder()
encoder_xavier_n.apply(initialize_weights_xavier_n)

weights_xavier_n = get_weight_list(encoder_xavier_n)
plot_weight_list(weights_xavier_n, "Xavier Initialization - Normal")
print_weight_stats(weights_xavier_n)

## He initialization 으로 파라미터 초기화 (relu에 적합)
def initialize_weights_he(m):
  if isinstance(m, nn.Linear):
      nn.init.kaiming_uniform_(m.weight, nonlinearity = 'relu')
      nn.init.constant_(m.bias, 0)

def initialize_weights_he_n(m):
  if isinstance(m, nn.Linear):
      nn.init.kaiming_normal_(m.weight, nonlinearity = 'relu')
      nn.init.constant_(m.bias, 0)

# weight 수집
def get_weight_list(model):
  weights_list = []
  for layer in model.modules():
    if isinstance(layer, nn.Linear):
      weights = layer.weight.data.cpu().flatten().numpy()
      weights_list.append(weights)
  return weights_list

# weight 시각화 함수
def plot_weight_list(weights_list, title="He Weights"):
    plt.figure(figsize=(len(weights_list) * 3, 4))
    for i, weights in enumerate(weights_list):
        plt.subplot(1, len(weights_list), i+1)
        plt.hist(weights, bins=50, density=True)
        plt.title(f"Layer {i+1}")
        plt.xticks([])
        plt.yticks([])
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# 통계 출력 함수
def print_weight_stats(weights_list):
    for i, w in enumerate(weights_list):
        print(f"[Layer {i+1}] mean: {np.mean(w):.4f}, std: {np.std(w):.4f}, min: {np.min(w):.4f}, max: {np.max(w):.4f}")

# He Uniform 초기화
encoder_he_u = Encoder()
encoder_he_u.apply(initialize_weights_he)

weights_he_u = get_weight_list(encoder_he_u)
plot_weight_list(weights_he_u, "He Initialization - Uniform")
print_weight_stats(weights_he_u)

# He Normal 초기화
encoder_he_n = Encoder()
encoder_he_n.apply(initialize_weights_he_n)

weights_he_n = get_weight_list(encoder_he_n)
plot_weight_list(weights_he_n, "He Initialization - Normal")
print_weight_stats(weights_he_n)

"""## 네트워크 모델 구조 확인

딥러닝 모델의 입출력 구조를 다음과 같이 확인한다.  여기서 중요한 사항은 다음을 확인하는 것이다.

1. 레이어의 연결구조,  
2. 레이어별 입출력 텐서의 차원,
3. 학습할 파라메터 개수
"""

!pip install torchsummary

#encoder에 he 초기화 적용
encoder = encoder_he_u
ae = AutoEncoder(encoder, dec)
ae = ae.to(device)

from torchsummary import summary

summary(ae, (1,28,28))

"""## 훈련 전, 네트워크 모델을 함수로서 활용
* AutoEncoder ae를 모델로 구성했기 때문에, 지금부터 함수로서 활용 가능
  + 단, ae 함수는 batch 단위로 수행됨을 명심할 것.  
    - 단순히, (1, 28, 28) -> ae -> (1, 28, 28)로 동작하지 않고,
    - batch 단위로 (?, 1, 28, 28) -> ae -> (?, 1, 28, 28)로 병렬처리됨.
* 지금은 훈련 전 네트웍이기 때문에 함수로서는 작동하지만 노이즈 출력만 나올 뿐, 정상적인 출력이 나오는 함수로 동작하지 않음.
"""

import matplotlib.pyplot as plt
from torchvision.transforms.functional import to_pil_image
import ipywidgets as widgets

print("He 초기화 적용")

def train_dataset_imshow(idx):
  (image, label) = train_dataset[idx]
  print('GT label:', label)

  X      = torch.unsqueeze(image, 0).to(device) # batch size = 1
  Y_pred = ae(X)

  input_img  = to_pil_image(X.squeeze())
  output_img = to_pil_image(Y_pred.squeeze())

  plt.subplot(121)
  plt.imshow(input_img, cmap='gray')

  plt.subplot(122)
  plt.imshow(output_img, cmap='gray')

  plt.show()

widgets.interact(train_dataset_imshow, idx=widgets.IntSlider(min=0, max=len(train_dataset)-1, continuous_update=False))

"""## Optimizer, loss 함수 설정
학습과정에서 사용할 optimizer, loss 함수를 설정한다.
* [optim module](https://pytorch.org/docs/stable/optim.html) 참고<br/>
optimizier 이론적 학습: [www](https://brunch.co.kr/@chris-song/50)<br/>
optimizer: torch.optim 모듈 내의  함수들을 다음의 예약어로 쓸 수 있다.
  + sgd = SGD
  + rmsprop = RMSprop
  + adagrad = Adagrad
  + adadelta = Adadelta
  + adam = Adam
  + adamax = Adamax
* [nn module](https://pytorch.org/docs/stable/nn.html) 참고 <br/>
loss:  torch.nn 모듈 내의  함수들을 다음의 예약어로 쓸 수 있다.
  + mse = MSELoss = mean_squared_error
  + mae = L1Loss = mean_absolute_error
  + kld = KLDivLoss = kullback_leibler_divergence
  + crossentropy = CrossEntropyLoss
"""

# optimizer,loss 설정
optimizer = torch.optim.Adam(ae.parameters(), lr=learning_rate)
criterion = nn.MSELoss()

"""## 딥러닝 모델 학습

pytorch에서는 tensorflow기반 코드와는 달리 `fit()` 함수를 직접 구현해야한다.
"""

def fit(data_loader, epochs):
  for epoch in range(epochs):
    now = time.time()
    avg_loss = 0
    total_batch = len(data_loader)

    for i, (batch_images, batch_labels) in enumerate(data_loader):

      X = batch_images.to(device)  # [-1, 1, 28, 28]

      # forward 단계
      # 1. input data를 모델에 통과시킵니다.
      # 2. loss를 계산합니다.
      Y_prediction = ae(X)
      loss = criterion(Y_prediction, X)

      # bacward 단계
      # 1. backprop 단계를 실행하기 전에 변화도를 0으로 만듭니다.
      # 2. 모델의 매개변수에 대한 손실의 변화도를 계산합니다.
      # 3. step 함수를 호출하면 매개변수가 갱신됩니다.
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      avg_loss += loss / total_batch

    print("[Epoch: {:>4}] \t loss = {:.4f} \t time = {:.4f}"
          .format(epoch + 1, avg_loss.data, time.time()-now))

  print("Learning Finished!")

"""fit 함수 실행"""

fit(train_loader, num_epochs)

"""### 트레이닝셋을 이용해 학습 후 결과 시각화
학습이 끝난 autoencoder에 대해 train_dataset에 대한 결과 시각화
"""

import matplotlib.pyplot as plt
from torchvision.transforms.functional import to_pil_image
import ipywidgets as widgets

def train_dataset_imshow(idx):
  (image, label) = train_dataset[idx]
  print('GT label:', label)

  X = torch.unsqueeze(image, 0).to(device) # batch size = 1
  Y_pred = ae(X)

  input_img  = to_pil_image(X.squeeze())
  output_img = to_pil_image(Y_pred.squeeze())

  plt.subplot(121)
  plt.imshow(input_img, cmap='gray')

  plt.subplot(122)
  plt.imshow(output_img, cmap='gray')

  plt.show()

widgets.interact(train_dataset_imshow, idx=widgets.IntSlider(min=0, max=len(train_dataset)-1, continuous_update=False))

import matplotlib.pyplot as plt
from torchvision.transforms.functional import to_pil_image
import ipywidgets as widgets

def train_dataset_imshow(idx):
  (image, label) = train_dataset[idx]
  print('GT label:', label)

  X = torch.unsqueeze(image, 0).to(device) # batch size = 1
  Y_pred = ae(X)

  input_img  = to_pil_image(X.squeeze())
  output_img = to_pil_image(Y_pred.squeeze())

  plt.subplot(121)
  plt.imshow(input_img, cmap='gray')

  plt.subplot(122)
  plt.imshow(output_img, cmap='gray')

  plt.show()

widgets.interact(train_dataset_imshow, idx=widgets.IntSlider(min=0, max=len(train_dataset)-1, continuous_update=False))

"""# 인코더 / 디코더 모델을 각각 따로 함수로서 활용하기
오토인코더 네트웍 전체가 아닌 `enc()` 부분과 `dec()` 부분을 각각 수행할 수 있다.
* 특정 예제에 대한 인코딩 결과와 디코딩 결과를 따로 확인한다.
"""

import matplotlib.pyplot as plt
from torchvision.transforms.functional import to_pil_image

idx = 128

(image, label) = test_dataset[idx]
print('GT label:', label)

### 아래에서 ae()가 사용되지 않고, enc()와 dec()가 사용되었음에 주목
X      = torch.unsqueeze(image, 0).to(device) # batch size = 1
z      = encoder(X)   # enc()만 사용해 입력 영상을 인코딩해 **latent code z**를 구함
Y_pred = dec(z)   # dec()만 사용해 latent code z를 디코딩해 출력 영상 Y_pred를 구함

print("latent code z: ", z)

input_img  = to_pil_image(X.squeeze())
output_img = to_pil_image(Y_pred.squeeze())

plt.subplot(121)
plt.imshow(input_img, cmap='gray')

plt.subplot(122)
plt.imshow(output_img, cmap='gray')

"""인코딩 결과와 유사한 좌표값을 디코딩에 보내도 유사한 결과가 나옴을 확인"""

import ipywidgets as widgets

u=widgets.FloatSlider(min=-10.0, max=10.0, orientation='horizontal')
v=widgets.FloatSlider(min=-10.0, max=10.0, orientation='vertical')

ui = widgets.HBox([u,v])

def z_test(u, v):
  z_test = np.array([[u,v]])

  print(z_test)

  z_test = torch.FloatTensor(z_test).to(device)
  img_gen = dec(z_test)

  img_gen = to_pil_image(img_gen.squeeze())

  plt.imshow(img_gen, cmap='gray')
  plt.show()

out = widgets.interactive_output(z_test, {'u': u, 'v': v})

display(ui, out)

"""## TODO
1. encoder를 거쳐 나온 representation z를 *GT label 별로 다른색*을 주어 **z의 분포를 가시화**한다.
2. z의 분포를 보고 encoder가 label 별로 discriminative한 representation을 만들어내는지 여부를 확인한다.
3. 만일 discriminative하지 않다면, autoencoder가 discriminative한 representation z를 학습하도록 z의 차원을 바꿔가면서 실험을 진행한다.
4. 결과에 대한 분석을 작성한다.

## 인코딩 결과 가시화
오토인코더의 encoder가 만들어 내는 representation인 z 값을 가시화 한다.
"""

# MNIST 데이터의 latent code 가시화
import matplotlib.pyplot as plt

# 10개의 color list를 만든다.
colors = ['red', 'green', 'blue', 'orange', 'violet',
          'khaki', 'pink', 'lightsalmon', 'darkseagreen', 'cyan']

# 테스트셋의 whole batch에 대한 결과 확인을 위한 dataloader 정의
whole_test_loader = DataLoader(dataset=test_dataset,
                               batch_size=len(test_dataset),
                               shuffle=False)
whole_test_images, whole_test_labels = next(iter(whole_test_loader))

whole_test_images = whole_test_images.to(device)
whole_test_labels = whole_test_labels.to(device)

## TODO: whole_train_images에 대한 인코딩 함수 enc()를 이용해 latent codes z 구하기
with torch.no_grad():
  z = encoder(whole_test_images)

# z 배열을 레이블 조건을 이용해 슬라이싱

# # TODO: 레이블 별로 다른 색을 이용해 가시화
for label in range(10):
  z_label = z[whole_test_labels==label]
  print(z_label.shape)
  plt.plot(z_label[:,0].detach().cpu().numpy(), z_label[:,1].detach().cpu().numpy(), marker = 'o', linestyle='', color = colors[label])

"""## TODO: 디코더를 이용한 Generative Model 구성
1. z 공간의 임의의 위치를 sampling한 후, 이를 decoder의 입력으로 주는 방식으로 generative model을 구축한다.
2. 결과로 만들어지는 이미지는 어떤 특징을 가지고 있는지 분석한다.
"""

## TODO
z = np.array([[-40, -20],
              [0, 60],
              [15, 0]
             ])

if use_cuda:
  z = torch.FloatTensor(z)
  z = z.to(device)
  result = dec(z).cuda()
else:
  z = torch.FloatTensor(z)
  result = dec(z)
print(z.shape)
print(result.shape)

"""결과 가시화"""

## TODO

# 로딩된 MNIST 데이터 가시화
import matplotlib.pyplot as plt
from torchvision.transforms.functional import to_pil_image

#result[0 ~ 2]는 decoder로부터 생성된 이미지
result_img_0 = to_pil_image(result[0].squeeze())
result_img_1 = to_pil_image(result[1].squeeze())
result_img_2 = to_pil_image(result[2].squeeze())

plt.subplot(131)
plt.imshow(result_img_0, cmap='gray')
plt.subplot(132)
plt.imshow(result_img_1, cmap='gray')
plt.subplot(133)
plt.imshow(result_img_2, cmap='gray')

"""Decoder를 이용한 generative model을 구성하기 위해,
2차원 latent space 상의 z 공간의 임의의 위치를 sampling, decoder의 입력으로 사용하였다.
decoder를 통해 '7', '0', '3' 에 해당하는 이미지가 생성되었다.
이미지는 MNIST 이미지와 같이 왜곡된 부분 없이 숫자로서의 형태 그대로 잘 복원되었다.

"""